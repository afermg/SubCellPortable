"""Main inference orchestration for SubCellPortable."""

import datetime
import logging
import os
import sys
import torch
import yaml
from torch.utils.data import DataLoader
from tqdm import tqdm

import inference
from vit_model import ViTPoolClassifier
from dataset import SubCellDataset, collate_fn
from config import SubCellConfig, PATH_LIST_CSV, LOG_FILE
from cli import parse_args
from model_loader import ensure_models_available
from output_handlers import CSVOutputHandler, H5ADOutputHandler, compute_top_predictions

# Set CUDA device ordering
os.environ["DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"


def setup_logging(log_file: str = LOG_FILE) -> logging.Logger:
    """Set up logging configuration.

    Args:
        log_file: Path to log file

    Returns:
        Configured logger
    """
    logging.basicConfig(
        filename=log_file,
        encoding="utf-8",
        format="%(levelname)s: %(message)s",
        filemode="w",
        level=logging.INFO,
    )
    console = logging.StreamHandler()
    logging.getLogger().addHandler(console)
    logger = logging.getLogger("SubCell inference")
    return logger


def load_config() -> SubCellConfig:
    """Load configuration from multiple sources.

    Configuration priority (highest to lowest):
    1. Command-line arguments
    2. config.yaml file
    3. Default values

    Returns:
        Merged configuration
    """
    # Start with defaults
    config_dict = {}

    # Load from config.yaml if exists
    if os.path.exists("config.yaml"):
        with open("config.yaml", "r") as f:
            yaml_config = yaml.safe_load(f)
            if yaml_config:
                config_dict.update(yaml_config)

    # Override with command-line arguments
    cli_args = parse_args()
    config_dict.update(cli_args)

    # Create config object (with validation)
    return SubCellConfig.from_dict(config_dict)


def setup_device(gpu_id: int, logger: logging.Logger) -> torch.device:
    """Set up computing device (CPU or GPU).

    Args:
        gpu_id: GPU ID to use (-1 for CPU)
        logger: Logger instance

    Returns:
        Configured torch device
    """
    if torch.cuda.is_available() and gpu_id != -1:
        device = torch.device(f"cuda:{gpu_id}")
        logger.info(f"Using GPU: cuda:{gpu_id}")
    else:
        if gpu_id != -1:
            logger.warning(f"GPU {gpu_id} requested but CUDA not available. Using CPU.")
        else:
            logger.info("Using CPU")
        device = torch.device("cpu")
    return device


def create_dataloader(
    csv_path: str,
    config: SubCellConfig,
    logger: logging.Logger
) -> DataLoader:
    """Create DataLoader for batch processing.

    Args:
        csv_path: Path to CSV file with image paths
        config: Configuration object
        logger: Logger instance

    Returns:
        Configured DataLoader

    Raises:
        FileNotFoundError: If CSV file doesn't exist
    """
    if not os.path.exists(csv_path):
        raise FileNotFoundError(
            f"Input file not found: {csv_path}. "
            f"Please create this file with your image paths."
        )

    dataset = SubCellDataset(
        csv_path,
        config.model_channels
    )

    dataloader = DataLoader(
        dataset,
        batch_size=config.batch_size,
        num_workers=config.num_workers,
        prefetch_factor=config.prefetch_factor,
        collate_fn=collate_fn,
        shuffle=False,
        pin_memory=torch.cuda.is_available(),
        persistent_workers=config.num_workers > 0,
    )

    logger.info(f"Processing {len(dataset)} images in batches of {config.batch_size}")
    logger.info(f"Using {config.num_workers} workers for data loading")

    return dataloader


def process_batch_results(
    batch_results: list,
    output_prefixes: list,
    classifier_paths: list,
    config: SubCellConfig,
) -> None:
    """Process and log batch results.

    Args:
        batch_results: List of (embedding, probabilities) tuples
        output_prefixes: List of output prefixes for logging
        classifier_paths: Classifier paths (None if embeddings_only)
        config: Configuration object
    """
    if config.quiet:
        return

    for i, (embedding, probabilities) in enumerate(batch_results):
        output_prefix = output_prefixes[i]
        log_message = f"- Saved results for {output_prefix}"

        if classifier_paths and probabilities is not None:
            _, _, top_3_names, _ = compute_top_predictions(probabilities)
            log_message += f", locations predicted [{top_3_names}]"

        config.log.info(log_message)


def run_inference() -> None:
    """Main inference pipeline."""
    # Track overall timing
    start_time = datetime.datetime.now()

    # Setup
    config = load_config()

    # Setup logging with output_dir if specified
    if config.output_dir:
        log_path = os.path.join(config.output_dir, "log.txt")
        os.makedirs(config.output_dir, exist_ok=True)
        logger = setup_logging(log_path)
    else:
        logger = setup_logging()

    config.log = logger

    # Log start
    logger.info("=" * 60)
    logger.info("SubCellPortable - Subcellular Localization Inference")
    logger.info("=" * 60)
    logger.info(f"Start: {start_time.strftime('%Y/%m/%d %H:%M:%S')}")
    logger.info("Parameters:")
    for key, value in config.to_dict().items():
        logger.info(f"  {key}: {value}")
    logger.info("-" * 60)

    try:
        # Ensure models are available
        classifier_paths, encoder_path, model_config = ensure_models_available(
            config.model_channels,
            config.model_type,
            config.embeddings_only,
            config.update_model,
        )

        # Load model
        model = ViTPoolClassifier(model_config)
        classifier_paths_for_loading = classifier_paths if classifier_paths is not None else []
        model.load_model_dict(encoder_path, classifier_paths_for_loading)
        model.eval()

        # Log mode
        if config.embeddings_only:
            logger.info("ðŸ” Running in EMBEDDINGS ONLY mode - no classification")
        else:
            logger.info("ðŸŽ¯ Running in FULL mode - embeddings + classification")

        # Setup device
        device = setup_device(config.gpu, logger)
        model.to(device)

        # Create dataloader
        dataloader = create_dataloader(PATH_LIST_CSV, config, logger)

        # Check CSV format and validate output_dir requirement
        uses_old_format = dataloader.dataset.uses_old_format

        if uses_old_format:
            logger.warning("âš ï¸  DEPRECATION WARNING: Your path_list.csv uses the old format with 'output_folder' column.")
            logger.warning("This format is deprecated and will be removed in a future version.")
            logger.warning("Please update to the new format: remove 'output_folder' column and use --output_dir instead.")
            logger.warning("See documentation for migration guide.")
        else:
            # New format requires output_dir
            if not config.output_dir:
                raise ValueError(
                    "output_dir is required when using new CSV format (without output_folder column). "
                    "Please specify via --output_dir or in config.yaml"
                )

        # Initialize output handlers
        csv_handler = None
        h5ad_handler = None

        if config.create_csv:
            csv_handler = CSVOutputHandler(has_classifier=classifier_paths is not None)

        if config.output_format == "combined":
            if uses_old_format:
                # Old format: use first row's output_folder
                first_item = dataloader.dataset.data_list[0]
                h5ad_handler = H5ADOutputHandler(first_item["output_folder"])
            else:
                # New format: use output_dir
                h5ad_handler = H5ADOutputHandler(config.output_dir)

        # Process batches
        pending_executors = []
        pending_futures = []

        for batch in tqdm(dataloader, desc="Processing batches", unit="batch"):
            images = batch["images"]
            output_prefixes = batch["output_prefixes"]

            # Prepare output paths based on format
            if uses_old_format:
                # Old format: use per-row output_folder from CSV
                output_folders = batch["output_folders"]
                # Create output directories
                for output_folder in set(output_folders):
                    os.makedirs(output_folder, exist_ok=True)
                # Prepare output paths
                output_paths = [
                    os.path.join(output_folders[i], output_prefixes[i])
                    for i in range(len(output_folders))
                ]
            else:
                # New format: use config.output_dir + output_prefix
                # output_prefix can include subdirectories (e.g., "experiment_A/cell1_")
                output_paths = []
                for prefix in output_prefixes:
                    full_path = os.path.join(config.output_dir, prefix)
                    # Create subdirectories if prefix contains them
                    output_dir_for_file = os.path.dirname(full_path)
                    if output_dir_for_file:
                        os.makedirs(output_dir_for_file, exist_ok=True)
                    output_paths.append(full_path)

            # Run inference
            inference_result = inference.run_model(
                model, images, device, output_paths,
                save_attention_maps=config.save_attention_maps,
                embeddings_only=config.embeddings_only,
                output_format=config.output_format,
                async_saving=config.async_saving
            )

            # Handle different return formats
            if config.output_format == "combined":
                batch_results = inference_result
            elif config.async_saving:
                batch_results, (executor, futures) = inference_result
                pending_executors.append(executor)
                pending_futures.extend(futures)
            else:
                batch_results = inference_result

            # Free GPU memory
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            # Extract embeddings and probabilities
            embeddings = [result[0] for result in batch_results]
            probabilities_list = [result[1] for result in batch_results]

            # Add to output handlers
            if csv_handler:
                csv_handler.add_batch(output_prefixes, embeddings, probabilities_list)

            if h5ad_handler:
                h5ad_handler.add_batch(output_prefixes, embeddings, probabilities_list)

            # Log progress
            process_batch_results(
                batch_results,
                output_prefixes,
                classifier_paths,
                config
            )

        # Wait for async saves to complete
        if config.async_saving and pending_futures:
            logger.info(f"Waiting for {len(pending_futures)} async save operations...")
            import concurrent.futures
            concurrent.futures.wait(pending_futures)
            for executor in pending_executors:
                executor.shutdown(wait=True)
            logger.info("All async saves completed")

        # Save accumulated outputs
        if csv_handler:
            if uses_old_format or not config.output_dir:
                # Old format or no output_dir: save to CWD
                csv_handler.save("result.csv")
            else:
                # New format: save to output_dir
                csv_path = os.path.join(config.output_dir, "result.csv")
                csv_handler.save(csv_path)

        if h5ad_handler:
            h5ad_handler.save(embeddings_only=config.embeddings_only)

        # Calculate timing statistics
        end_time = datetime.datetime.now()
        elapsed = end_time - start_time
        total_images = len(dataloader.dataset)
        images_per_sec = total_images / elapsed.total_seconds() if elapsed.total_seconds() > 0 else 0

        # Log success summary
        logger.info("-" * 60)
        logger.info("âœ“ Processing completed successfully")
        logger.info(f"Total images processed: {total_images}")
        logger.info(f"Total time: {elapsed}")
        logger.info(f"Average speed: {images_per_sec:.2f} images/sec")
        logger.info(f"Average time per image: {elapsed.total_seconds()/total_images:.4f} sec")

        # Log output location
        if config.output_dir:
            logger.info(f"Output saved to: {os.path.abspath(config.output_dir)}")
            if config.create_csv:
                logger.info(f"  - result.csv")
            if config.output_format == "combined":
                logger.info(f"  - embeddings.h5ad")
            logger.info(f"  - log.txt")
            if config.output_format == "individual":
                logger.info(f"  - {total_images} individual files (*_embedding.npy, etc.)")

        # Clean up
        del dataloader

    except FileNotFoundError as e:
        logger.error("-" * 60)
        logger.error(f"âœ— File not found: {e}")
        logger.error("Please check that all input files exist.")
        raise
    except ValueError as e:
        logger.error("-" * 60)
        logger.error(f"âœ— Configuration error: {e}")
        logger.error("Please check your configuration and CSV format.")
        raise
    except RuntimeError as e:
        logger.error("-" * 60)
        logger.error(f"âœ— Runtime error: {e}")
        if "out of memory" in str(e).lower():
            logger.error("Suggestion: Try reducing batch_size in config.yaml")
        raise
    except Exception as e:
        logger.error("-" * 60)
        logger.error(f"âœ— Unexpected error: {e}")
        logger.error(f"Error type: {type(e).__name__}")
        raise

    finally:
        end_time = datetime.datetime.now()
        logger.info("=" * 60)
        logger.info(f"End: {end_time.strftime('%Y/%m/%d %H:%M:%S')}")
        logger.info("=" * 60)


if __name__ == "__main__":
    run_inference()
